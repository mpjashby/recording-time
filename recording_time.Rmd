---
title             : "Incorporating time into studying crime and place"

author: 
  - name          : "Matthew P J Ashby"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "35 Tavistock Square, London WC1H 9EZ"
    email         : "matthew.ashby@ucl.ac.uk"

affiliation:
  - id            : "1"
    institution   : "Jill Dando Institute of Security and Crime Science, University College London"

# keywords          : "keywords"
wordcount         : "`scales::comma(wordcountaddin::word_count())`"

bibliography      : "../bibliography.bib"

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---


```{r setup, include = FALSE}
# load packages
library("bookdown")
library("circular")
library("cowplot")
library("feasts")
library("lubridate")
library("papaja")
library("sf")
library("tsibble")
library("tidyverse")

# suppress printing of code from code chunks
knitr::opts_chunk$set(cache = TRUE, include = FALSE)
# knitr::knit_theme$set("greyscale0")

# load helpers
source("../helpers.R")
```
```{r create functions, include = FALSE}
ts_count <- function(data, var) {
  as_tibble(fill_gaps(as_tsibble(count(data, {{var}}), index = {{var}})))
}

# the Washington DC data is recorded in the Maryland State Plane, which seems to
# cause a problem with st_transform() because of the bug at
# https://github.com/r-spatial/mapview/issues/300 so we instead use the PROJ
# string from https://spatialreference.org/ref/epsg/nad83-maryland/ rather than 
# the EPSG code 102685
msp_proj <- "+proj=lcc +lat_1=38.3 +lat_2=39.45 +lat_0=37.66666666666666 +lon_0=-77 +x_0=399999.9999999999 +y_0=0 +ellps=GRS80 +datum=NAD83 +to_meter=0.3048006096012192 no_defs"

```


```{r load data, include = FALSE}
if (!file.exists(here::here("dc_crime_data.csv.gz"))) {
  
  # download data file
  zip_file <- tempfile(fileext = ".zip")
  download.file(
    "https://github.com/ElizabethGroff/CrimeAndPlaceMethods/raw/master/WashingtonDC_Data/wash_dc_incidents_2016.zip", 
    zip_file
  )
  unzip(zip_file, exdir = tempdir())
  
  # load file
  data <- tempdir() %>% 
    dir("wash_dc_incidents_2016.shp$", full.names = TRUE) %>% 
    sf::read_sf() %>% 
    sf::st_set_geometry(NULL) %>% 
    as_tibble() %>% 
    janitor::clean_names() %>% 
    mutate(
      from_date_time = parse_date_time(paste(from_date, from_time), "Ymd HM"),
      to_date_time = parse_date_time(paste(to_date, to_time), "Ymd HM")
    )
  
  # save data
  write_csv(data, here::here("dc_crime_data.csv.gz"))
  
} else {
  
  data <- read_csv(here::here("dc_crime_data.csv.gz"))
  
}
```


```{r load district boundaries, include=FALSE}
# get districts
districts_file <- tempfile(fileext = ".zip")
download.file(
  "https://opendata.arcgis.com/datasets/d2a63e5246ff41bdaca8ea9be95c8a4b_9.zip",
  districts_file
)
unzip(districts_file, exdir = tempdir())
districts <- str_glue("{tempdir()}/Police_Districts.shp") %>% 
  st_read() %>% 
  st_transform(crs = msp_proj)
```



# Introduction

Understanding how crime varies over time is just as important as understanding how it varies between places. Very few places are crime hotspots all the time – business districts might be hotspots of pickpocketing in the daytime but deserted at night, while a nearby entertainment district may be quiet in the daytime but a violence hotspot at night.

Temporal variation in crime frequency is widespread. There are often long-term trends, such as the crime drop that occurred in many countries from the 1990s onward. Trends also exist at smaller spatial scales: micro places such as street segments have different trajectories of crime, which drive crime trends at larger spatial scales [@Andresen:2017aa].

(ref:seasonal) Difference from mean frequency crimes in Washington, DC, 2016

```{r seasonal, include=TRUE, fig.cap="(ref:seasonal)"}
seasonal <- data %>% 
  filter(!offense %in% c("ARSON", "HOMICIDE", "SEX ABUSE")) %>% 
  mutate(
    offense = recode(
      str_to_lower(offense),
      "assault w/dangerous weapon" = "assault with a dangerous weapon",
      "motor vehicle theft" = "theft of motor vehicle",
      "theft f/auto" = "theft from motor vehicle",
      "theft/other" = "other theft"
    ),
    week = yearweek(from_date)
  ) %>% 
  count(offense, week) %>% 
  as_tsibble(index = week, key = offense) %>% 
  fill_gaps(n = 0) %>% 
  as_tibble() %>% 
  group_by(offense) %>% 
  mutate(
    # calculate mean here because it is needed after ungroup() below
    mean = mean(n),
    diff = (n - mean) / mean,
    # next line from https://stackoverflow.com/a/50759346/8222654
    diff_ma = raster::movingFun(diff, 12, circular = TRUE),
    # median_diff = median(abs(diff), na.rm = TRUE)
    sd = sd(n, na.rm = TRUE),
    week = as_date(week)
  ) %>% 
  ungroup() %>% 
  mutate(offense = str_glue("{offense} (mean = {scales::comma(mean)})"))

ggplot(seasonal, aes(week, diff)) +
  geom_col(fill = "grey70") +
  geom_line(aes(y = diff_ma, colour = "ma"), size = 0.5, 
            key_glyph = "timeseries") +
  # geom_smooth(method = "loess", formula = "y ~ x", se = FALSE) +
  scale_x_date(date_breaks = "2 months", date_labels = "%b") +
  scale_y_continuous(
    breaks = seq(-0.5, 0.5, by = 0.25),
    labels = scales::percent_format(accuracy = 1)
  ) +
  scale_colour_manual(
    values = c("ma" = "black"),
    labels = c("ma" = "12-week center-weighted moving average")
  ) +
  facet_wrap(vars(offense), labeller = label_wrap_gen()) +
  labs(
    # caption = "* median absolute difference between each weekly count and the mean weekly count",
    x = NULL, 
    y = "difference between weekly count and mean weekly count",
    colour = NULL
  ) +
  theme_ashby() +
  theme(panel.grid.minor.y = element_blank())

ggsave("figure_14-1.eps", width = 7, height = 7 * 0.667, units = "in")
ggsave("figure_14-1.pdf", width = 7, height = 7 * 0.667, units = "in")
# ggsave("figure_14-1.tiff", width = 7, height = 7 * 0.667, units = "in", 
#        dpi = 800)
```

Non-trend temporal variations can be divided into two categories: cyclical variations (including seasonal patterns) and short-term fluctuations (including random 'noise'). Figure \@ref(fig:seasonal) shows how weekly counts of various crimes in Washington, DC, differ from the mean weekly count. Some types (such as assault with a dangerous weapon) exhibit strong seasonal patterns, while others (such as robbery) do not. There are also substantial short-term variations.

Focusing only on where crime happens, rather than analyzing when and where together, leads to crime-prevention failures. Many police (the author included) will have been sent to patrol a hotspot at a time when no crimes had ever occurred there, because a crime analyst had considered where crimes concentrated, but not when. Equally, a theory explaining why crime concentrates at places but not at times is likely to lack predictive power.

```{r}
seasonal <- data %>% 
  filter(!offense %in% c("ARSON")) %>% 
  select(offense, from_date) %>% 
  # convert dates to angles
  mutate(angle = (yday(from_date) / max(yday(from_date))) * 360) %>% 
  # nest data (rather than group) because number of crimes in each group is
  # not equal to the number of density estimates
  nest(dates = c(from_date, angle)) %>% 
  mutate(
    # convert angles to a circular density estimate
    circular = map(dates, ~ circular(.$angle, units = "degrees", zero = pi/2,
                                     rotation = "clock")),
    density = map(circular, ~ density(., bw = bw.nrd.circular(.), adjust = 5, 
                                      n = 366)),
    # extract the angles at which the density estimates were made
    density_x = map(density, ~ as.numeric(pluck(., "x"))),
    # extract the density estimates
    density_y = map(density, ~ as.numeric(pluck(., "y")))
  ) %>% 
  select(offense, density_x, density_y) %>% 
  unnest(cols = c(density_x, density_y)) %>% 
  mutate(
    # correct these for circular.density() returning estimates in degrees but
    # counting anti-clockwise starting from 90ª
    density_x = ifelse(density_x < 0, density_x + 360, density_x),
    yday = (density_x / 360) * max(yday(data$from_date)),
    date = min(data$from_date) + days(round(yday))
  ) %>% 
  arrange(offense, date) %>% 
  mutate(
    offense = str_replace(str_replace(str_to_lower(offense), " w/", " with a "), 
                          " f/", " from ")
  )

seasonal_mean <- seasonal %>% 
  group_by(offense) %>% 
  summarise(density_y = mean(density_y))

# ggplot(seasonal, aes(date, density_y)) +
#   geom_area(position = "identity") +
#   geom_hline(
#     aes(yintercept = density_y), 
#     data = seasonal_mean, 
#     colour = chart_elements$average_line_colour, 
#     linetype = chart_elements$average_line_linetype
#   ) +
#   scale_x_date(date_breaks = "2 months", date_labels = "%b") +
#   scale_y_continuous(limits = c(0, NA)) +
#   facet_wrap(vars(offense)) +
#   theme_ashby() +
#   theme(
#     panel.grid = element_blank(),
#     axis.text.y = element_blank()
#   )
```


# Why studying temporal data can be challenging

Temporal data can be challenging for several reasons. Dates can be stored in many formats: 17 January 1981 can be 1/17/1981 in the United States, 17.01.81 in Germany and 17/01/1981 in France. Many formats are ambiguous – does 10/02 represent 10 February or 2 October, or even October 2002? Is 3.45 in the early hours of the morning or mid-afternoon? Dates and times should be stored in an unambiguous format such as 1981-01-17 13:43, with hours represented using the 24-hour clock. Date-format problems can be hard to detect: even if an analyst knows what a value represents, collaborators or software may make different assumptions. For example, Microsoft Excel guesses (often incorrectly) whether numbers in a cell represent a date.


## Imprecision in temporal data

(ref:minutes) Burglaries reported by minute of the day in Washington, DC, 2016

```{r minutes, include=TRUE, fig.cap="(ref:minutes)"}
data %>% 
  filter(offense == "ROBBERY") %>% 
  mutate(
    from_time = parse_date_time(paste("2000-01-01", from_time), "Ymd HMS")
  ) %>% 
  count(from_time) %>% 
  mutate(on_hour = factor(case_when(
    str_detect(from_time, ":00:00$") ~ "on the hour",
    str_detect(from_time, ":30:00$") ~ "on the half hour",
    TRUE ~ "other"
  ), levels = c("on the hour", "on the half hour", "other"))) %>% 
  ggplot(aes(from_time, n, colour = on_hour, fill = on_hour)) + 
  geom_point(shape = 21, size = 0.75) +
  annotate(
    "curve", 
    x = ymd_hm("2000-01-01 03:15"), 
    y = 22, 
    xend = ymd_hm("2000-01-01 05:15"), 
    yend = 22,
    arrow = chart_elements$label_arrow,
    colour = chart_elements$label_line_colour, 
    curvature = chart_elements$label_line_curvature * -1
  ) +
  annotate(
    "label", 
    x = ymd_hm("2000-01-01 05:15"), 
    y = 22, 
    label = str_wrap(str_glue("robberies were much more likely to be ", 
                              "recorded as occurring on the hour or half hour ",
                              "than would be expected by chance"), 45), 
    colour = chart_elements$label_text_colour, 
    hjust = 0, 
    label.size = NA,
    lineheight = chart_elements$label_text_lineheight, 
    size = chart_elements$label_text_size
  ) +
  scale_x_datetime(date_breaks = "1 hour", date_labels = "%H", 
                   expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(
    breaks = seq(0, 25, by = 5),
    expand = expansion(mult = c(0.02, 0.02))
  ) +
  scale_colour_manual(
    name = "minute crime recorded as occurring: ",
    values = c("on the hour" = "black", "on the half hour" = "grey60", 
               "other" = "grey60")
  ) +
  scale_fill_manual(
    name = "minute crime recorded as occurring: ",
    values = c("on the hour" = "black", "on the half hour" = "grey60", 
               "other" = "white")
  ) +
  theme_ashby() +
  labs(
    x = "time of day",
    y = "number of robberies"
  ) +
  theme(
    legend.background = element_rect(colour = NA, fill = "white"),
    legend.direction = "horizontal",
    legend.justification = c(0, 1),
    legend.key.width = unit(0.5, "lines"),
    legend.position = c(0.02, 1),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 9),
    panel.grid.major.x = element_line(),
    panel.grid.minor.x = element_line(linetype = "21"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
  )

ggsave("figure_14-2.eps", width = 7, height = 7 * 0.5, units = "in")
ggsave("figure_14-2.pdf", width = 7, height = 7 * 0.5, units = "in")

# ggsave("figure_14-2.eps", width = 159, height = 159 * 0.6, units = "mm")
# ggsave("figure_14-2.tiff", width = 7, height = 7 * 0.6, units = "in", 
#        dpi = 800)
```

```{r on-the-hour simulation, eval=FALSE}
minute_counts <- data %>% 
  filter(offense == "ROBBERY") %>% 
  mutate(
    from_time = parse_date_time(paste("2000-01-01", from_time), "Ymd HM")
  ) %>% 
  count(from_time) %>% 
  tsibble::as_tsibble(index = from_time) %>% 
  tsibble::fill_gaps(n = 0) %>% 
  as_tibble()

prob_on_hour <- minute_counts %>% 
  mutate(on_hour = str_detect(from_time, ":00:00$")) %>% 
  top_n(24, n) %>% 
  count(on_hour) %>% 
  mutate(prob = n / sum(n)) %>% 
  filter(on_hour == TRUE) %>% 
  pull(n)

minute_simulations <- map_dfr(1:10000, function (i) {

  minute_counts %>% 
    mutate(order = runif(n())) %>% 
    arrange(order) %>% 
    mutate(
      minute = 1:(24 * 60) - 1,
      on_hour = minute %% 60 == 0
    ) %>% 
    arrange(desc(n)) %>% 
    mutate(
      rank = row_number()
    ) %>% 
    filter(rank <= 24) %>% 
    count(on_hour)
  
}, .id = "iteration") %>% 
  pivot_wider(names_from = on_hour, values_from = n) %>% 
  replace_na(list(`TRUE` = 0, `FALSE` = 0)) %>% 
  mutate(prob = `TRUE` / 24)

quantile(minute_simulations$`TRUE`, c(0.975, 0.995, 0.9995))
```


Temporal data is often imprecise. Some values (such as when a call to 911 was made) are known to the second, but others (such as the time of death of a decomposed body) might only be known to the nearest day or week. A common example is the times of crimes reported by victims. A person who has been robbed on the street might know it happened 'about a half-hour ago', leading either victims or officers to round-off the time at which they record a crime as occurring. Figure \@ref(fig:minutes) shows that robberies in Washington, DC are much more likely to be reported as occurring on the hour than would be expected by chance. An extreme version of this problem is when a victim doesn't know when a crime occurred, only the earliest and latest possible times – dealing with this using aoristic analysis is the subject of Chapter 14.2.



## Choosing a temporal scale

One reason temporal data are challenging is the conceptual nature of time. Time is linear: the instant in time at `r strftime(now(), "%T on %A %e %B %Y")` will never be repeated in the Gregorian Calendar. But time also has overlapping cycles: a `r scales::ordinal(as.numeric(strftime(now(), "%e")))` of `r strftime(now(), "%B")` every year, a `r scales::ordinal(day(now()))` day every month, `r strftime(now(), "%A")` every week, `r strftime(now(), "%H:%M")` every day and `r minute(now())` `r ifelse(minute(now()) == 1, "minute", "minutes")` past every hour. There are also exceptions to these rhythms: February in some years has 29 days (and the year 366 days); some days are 23-hours long and others 25-hours long due to daylight savings changes. 

(ref:scales) Thefts from automobiles in Washington, DC, 2016, using different temporal units of analysis

```{r scales, include=TRUE, fig.cap="(ref:scales)"}
auto_theft <- filter(data, offense == "THEFT F/AUTO")

plot_year <- auto_theft %>% 
  ts_count(from_date) %>% 
  ggplot(aes(from_date, n)) +
  geom_col(fill = "grey70", position = "identity", width = 1) +
  geom_smooth(aes(colour = "black"), method = "loess", formula = "y ~ x", 
              se = FALSE, span = 0.25, size = 0.5) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b", 
               expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(limits = c(0, NA), n.breaks = 5, 
                     expand = expansion(mult = c(0, 0))) +
  scale_colour_manual(
    values = c("black" = "black"),
    labels = c("black" = "smoothed count of crimes")
  ) +
  labs(x = NULL, y = "number of crimes", colour = NULL) +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.title.y = element_text(hjust = 0),
    legend.background = element_rect(size = 0, colour = NA, fill = "white"),
    legend.position = c(0, 1),
    legend.justification = c(0, 1),
    panel.grid.major.x = element_line(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  )

plot_month <- auto_theft %>% 
  mutate(date = as_date(from_date) - mday(from_date) + days(1)) %>% 
  count(date) %>% 
  ggplot(aes(date, n)) +
  geom_col(fill = "grey70") +
  scale_x_date(date_breaks = "2 months", date_labels = "%b", 
               expand = expansion(mult = c(0.01, 0.02))) +
  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format(),
                     expand = expansion(mult = c(0, 0))) +
  labs(x = NULL, y = "number of crimes") +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.title.y = element_text(hjust = 0),
    panel.grid.minor = element_blank()
  )

plot_day <- auto_theft %>% 
  mutate(
    weekday = wday(from_date, label = FALSE, week_start = 1),
    # next line from https://stackoverflow.com/a/26129060/8222654
    date = as.Date("1-1-2000","%u-%W-%Y") + days(weekday - 1)
  ) %>% 
  count(date) %>% 
  ggplot(aes(date, n)) +
  geom_col(fill = "grey70") +
  scale_x_date(date_breaks = "1 day", date_labels = "%a", 
               expand = expansion(mult = c(0.01, 0.02))) +
  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format(),
                     expand = expansion(mult = c(0, 0))) +
  labs(x = NULL, y = NULL) +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.title.y = element_text(hjust = 0),
    panel.grid.minor = element_blank()
  )

plot_hour <- auto_theft %>% 
  mutate(
    date_time = ymd_h(paste("2000-01-01", hour(from_time))),
    date_time = as_datetime(ifelse(hour(from_time) < 5, date_time + days(1),
                                   date_time))
  ) %>% 
  count(date_time) %>% 
  ggplot(aes(date_time, n)) +
  geom_col(fill = "grey70") +
  scale_x_datetime(
    breaks = ymd_hm(c("2000-01-01 05:00", "2000-01-01 11:00", 
                      "2000-01-01 17:00", "2000-01-01 23:00")), 
    date_labels = "%H:%M",
    expand = expansion(mult = c(0.01, 0.02))
  ) +
  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format(),
                     expand = expansion(mult = c(0, 0.2))) +
  labs(x = NULL, y = NULL) +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.title.y = element_text(hjust = 0),
    panel.grid.minor = element_blank()
  )

plot_day_hour <- auto_theft %>% 
  mutate(
    weekday = wday(from_date, label = FALSE, week_start = 1),
    # next line from https://stackoverflow.com/a/26129060/8222654
    date = as.Date("1-1-2000","%u-%W-%Y") + days(weekday - 1),
    date_hour = parse_date_time(paste(date, hour(from_time)), "Ymd H")
  ) %>% 
  count(date_hour) %>% 
  ggplot(aes(date_hour, n)) +
  geom_col(fill = "grey70") +
  scale_x_datetime(date_breaks = "1 day", date_labels = "%a", 
                   expand = expansion(mult = c(0.01, 0.02))) +
  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format(),
                     expand = expansion(mult = c(0, 0))) +
  labs(x = NULL, y = "number of crimes") +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.title.y = element_text(hjust = 0),
    panel.grid.major.x = element_line(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  )

plot_grid(
  plot_year, 
  plot_grid(plot_month, plot_day, plot_hour, nrow = 1, 
            labels = c("B", "C", "D"), label_fontface = "plain"), 
  plot_day_hour,
  nrow = 3, 
  labels = c("A", "", "E"),
  label_fontface = "plain"
)

ggsave("figure_14-3.eps", width = 7, height = 7, units = "in")
ggsave("figure_14-3.pdf", width = 7, height = 7, units = "in")

# ggsave("figure_14-3.eps", width = 159, height = 159, units = "mm")
# ggsave("figure_14-3.tiff", width = 7, height = 7, units = "in", dpi = 800)
```


These complications make it important to choose an appropriate temporal scale for analysis. Figure \@ref(fig:scales) shows the frequency of thefts from automobiles in Washington, DC, at different temporal scales. Panel A shows daily frequency. Panel B shows the same data, but aggregated to monthly counts. This is a common way to present trends, but monthly counts should almost always be avoided. Not all months are the same length (March is `r scales::percent((31 - 28) / 28, accuracy = 1)` longer than February), so it's likely Panel B would show month-by-month variations even if crime occurred at a constant rate. Months also don't include equal numbers of each weekday, which is important when (as shown in Panel C) crime concentrates on certain days. The only reason to use monthly counts of crime is if data are only available in that format (e.g. administrative data published monthly). Otherwise, use weeks or multiples of weeks as a unit of analysis.

Panel C shows thefts on each day of the week. There is a weekly pattern, with thefts peaking on Fridays and Saturdays. This is obscured by noise in Panel A, and hidden within monthly counts in Panel B – it is often useful to visualize patterns at multiple temporal scales.

Panel D shows thefts by hour of the day. The horizontal axis begins at 05:00, following @Felson:2003fu, since human activity patterns rarely begin or end neatly at midnight. Thefts have two peaks, one around midday and one around 18:00, with thefts gradually tailing off either side. With a plot axis beginning at midnight, the increase before midday would be obvious but the gradual decrease after midnight could have been missed, being split between the two sides of the plot.

Panel E also shows hourly crime counts, but for hours of the week rather than the day. Combining units of analysis in this way can be useful to identify how (for example) hourly patterns vary between days. This additional detail, however, comes at the cost of statistical noise in the data being more apparent, making some trends more difficult to identify. Choosing the appropriate trade off between noise and temporal detail requires judgment, which can often be helped by experimenting with multiple temporal scales to find the best balance between factors. It is also possible to use smoothing techniques (as in Panel A) such as moving averages or LOESS smoothing [@Buskirk:2013aa] to identify patterns amid noise.

The appropriate temporal scale also depends on the question being asked. Identifying changes in crime over several years has little need for hourly data, but might need weekly counts or counts for 28-day periods (to avoid the problems of monthly periods). It is often worthwhile considering a smaller temporal unit than initial instinct suggests, since (as can be seen by comparing panels C and E a scale that is too large masks important variations. This does not mean always plotting data at the smallest available scale -- the non-nested cycles seen in temporal data mean choosing a scale is more complicated than moving up and down the 'cone of resolution' used for spatial data [@Brantingham:1976aa]. For example, Panel A shows the data in its rawest form, but arguably conveys less information that the other panels that show aggregated data.


## Handling temporal ordering and autocorrelation

One feature of temporal data is that each data point has an inherent serial order relative to other points. So whereas (for example) survey respondents are typically independent of one another, repeated measures of a quantity at different time points are not. 
<!-- This has implications for various research methods. For example, it is increasingly common to assess the validity of statistical models by dividing the available data at random into a sample used to fit (or train) the model and a separate sample used to test it [@James:2013aa]. For time-series data, this approach much be modified to account for the temporal ordering of the data. -->
One consequence of this is temporal (or serial) autocorrelation: observations of data taken close together (e.g. the count of crime last week and the count this week) are usually more similar than observations taken further apart (e.g. the count last week compared to the count six months ago). As Panel A of Figure \@ref(fig:autocorrelation) shows, high weekly counts of vehicle theft are more likely to be followed by other high counts, and low counts more likely to be followed by other low counts (in this case, because of seasonality).

(ref:autocorrelation) Motor vehicle thefts by week in Washington, DC, 2016

```{r autocorrelation, include=TRUE, fig.cap="(ref:autocorrelation)"}
ac_data <- data %>% 
  filter(offense == "MOTOR VEHICLE THEFT") %>% 
  mutate(week = yearweek(from_date)) %>% 
  filter(year(week) == 2016) %>% 
  count(week) %>% 
  mutate(
    mean_n = mean(n),
    week = as_date(week)
  )

ac_test <- Box.test(ac_data$n, type = "Ljung-Box")

ac_plot <- ggplot(ac_data) +
  geom_ribbon(aes(week, ymin = mean_n, ymax = n), fill = "grey85") +
  geom_line(aes(week, mean_n), colour = chart_elements$average_line_colour, 
            linetype = chart_elements$average_line_linetype) +
  geom_line(aes(week, n)) +
  annotate(
    "curve", 
    x = ymd("2016-05-15"), 
    y = ac_data$mean_n[1] - 0.5, 
    xend = ymd("2016-05-30"), 
    yend = ac_data$mean_n[1] * 0.8,
    arrow = chart_elements$label_arrow,
    colour = chart_elements$label_line_colour, 
    curvature = chart_elements$label_line_curvature
  ) +
  annotate(
    "label",
    x = ymd("2016-05-30"),
    y = ac_data$mean_n[1] * 0.8,
    label = str_wrap(str_glue("mean {scales::comma(ac_data$mean_n[1])} ",
                              "crimes per week"), 15),
    colour = chart_elements$label_text_colour,
    hjust = 0,
    vjust = 1,
    label.size = NA,
    lineheight = chart_elements$label_text_lineheight,
    size = chart_elements$label_text_size
  ) +
  scale_x_date(date_breaks = "2 months", date_labels = "%b", 
               expand = expansion(mult = c(0.01, 0.02))) +
  scale_y_continuous(labels = scales::comma_format(), limits = c(0, NA),
                     expand = expansion(mult = c(0, 0.01))) +
  labs(x = NULL, y = "number of crimes") +
  theme_ashby() +
  theme(
    axis.title.y = element_text(hjust = 0),
    panel.grid = element_blank()
  )


# the following code using parts of View(forecast:::autoplot.acf)
acf_test <- acf(ac_data$n, plot = FALSE)
acf_test_ci <- qnorm((1 + 0.95)/2)/sqrt(acf_test$n.used)
acf_plot <- acf_test %>% 
  pluck("acf") %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  mutate(lag = row_number() - 1) %>% 
  select(lag, acf = V1) %>% 
  ggplot() +
  geom_hline(yintercept = 0, colour = "grey50") +
  geom_segment(aes(lag, acf, xend = lag, yend = 0)) +
  geom_hline(
    yintercept = c(-acf_test_ci, acf_test_ci),
    colour = chart_elements$average_line_colour,
    linetype = chart_elements$average_line_linetype
  ) +
  annotate(
    "curve", 
    x = 8.5, 
    y = acf_test_ci, 
    xend = 9.5, 
    yend = acf_test_ci * 1.6,
    arrow = chart_elements$label_arrow,
    colour = chart_elements$label_line_colour, 
    curvature = chart_elements$label_line_curvature * -1
  ) +
  annotate(
    "label",
    x = 9.5,
    y = acf_test_ci * 1.6,
    label = str_wrap("95% confidence interval", 15),
    colour = chart_elements$label_text_colour,
    hjust = 0,
    label.size = NA,
    lineheight = chart_elements$label_text_lineheight,
    size = chart_elements$label_text_size
  ) +
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(n.breaks = 7) +
  scale_fill_manual(
    values = c("95" = "grey85", "99" = "grey95"),
    labels = c("95" = "95%", "99" = "99%")
  ) +
  labs(
    title = NULL, 
    x = "lag", 
    y = "autocorrelation function (ACF)",
    fill = "confidence interval"
  ) +
  theme_ashby() +
  theme(
    axis.title.y = element_text(hjust = 0),
    panel.grid.minor.y = element_blank()
  )

plot_grid(ac_plot, acf_plot, align = "h", ncol = 2, labels = c("A", "B"), 
          label_fontface = "plain")

ggsave("figure_14-4.eps", width = 7, height = 7 * 0.5, units = "in")
ggsave("figure_14-4.pdf", width = 7, height = 7 * 0.5, units = "in")

# ggsave("figure_14-4.eps", width = 159, height = 159 * 0.6, units = "mm")
# ggsave("figure_14-4.tiff", width = 7, height = 7 * 0.6, units = "in", 
#        dpi = 800)
```

Temporal autocorrelation is challenging because many statistical techniques produce misleading results if the assumption that observations are independent is violated. In particular, $t$-tests and linear regressions may be inaccurate [@De-Boef:2004aa]. Autocorrelation can be identified using tests such as the Ljung-Box test [@Ljung:1978aa], which shows that there is significant autocorrelation in the weekly counts of vehicle theft in Panel A of Figure \@ref(fig:autocorrelation) ($Q = `r scales::number(ac_test$statistic, accuracy = 0.1)`$, $`r scales::pvalue(ac_test$p.value, add_p = TRUE)`$). Temporal autocorrelation can also be analyzed graphically using an autocorrelation function (ACF) plot [@Box:1970aa]. In Panel B of Figure \@ref(fig:autocorrelation) there is significant autocorrelation present (i.e. ACF values outside the confidence interval) for the first several lags.

Autocorrelation can be handled by using statistical methods appropriate for temporal data. Autoregressive integrated moving average (ARIMA) models are widely implemented in statistical software and commonly used in studying temporal crime patterns. These models can handle many common features of temporal data, including seasonal variation. 
<!-- An example ARIMA model is used in the supplementary material for this chapter, but for an accessible introduction to ARIMA models in criminology, see @Dugan:2010aa. For a more-formal explanation see Chapter 4 of @Box:2016aa. -->



# Incorporating time into spatial analysis

(ref:districts) Assaults with dangerous weapons by hour of the week in Washington, DC, 2016

```{r districts, include=TRUE, fig.cap="(ref:districts)"}
nbhd_counts <- data %>% 
  filter(
    district %in% c(1, 6),
    offense == "ASSAULT W/DANGEROUS WEAPON"
  ) %>% 
  mutate(
    district = scales::ordinal(district),
    weekday = wday(from_date, label = FALSE, week_start = 1),
    # next line from https://stackoverflow.com/a/26129060/8222654
    date = as.Date("1-1-2000","%u-%W-%Y") + days(weekday - 1),
    date_hour = parse_date_time(paste(date, hour(from_time)), "Ymd H")
  )

ggplot() +
  geom_density(
    aes(date_hour, colour = district, fill = district),
    data = filter(nbhd_counts, district == "1st"), 
    adjust = 0.2
  ) +
  ggpattern::geom_density_pattern(
    aes(date_hour, pattern_colour = district, pattern_fill = district),
    data = filter(nbhd_counts, district == "6th"), 
    adjust = 0.2, 
    colour = "grey70",
    pattern = "stripe", 
    pattern_angle = 45, 
    pattern_density = 0.05,
    pattern_spacing = 0.01
  ) +
  scale_x_datetime(date_breaks = "1 day", date_labels = "%a", 
                   expand = expansion(mult = c(0.01, 0.02))) +
  scale_y_continuous(limits = c(0, NA), labels = scales::comma_format(),
                     expand = expansion(mult = c(0, 0))) +
  scale_fill_manual(
    values = c("1st" = "grey90", "6th" = "grey75"),
    labels = c(
      "1st" = "First (downtown)   ",
      "6th" = "Seventh (residential)"
    ),
    aesthetics = c("colour", "fill", "pattern_colour", "pattern_fill")
  ) +
  labs(
    x = NULL, 
    y = "density of crimes", 
    colour = "Metropolitan Police District", 
    fill = "Metropolitan Police District",
    pattern_colour = "Metropolitan Police District", 
    pattern_fill = "Metropolitan Police District"
  ) +
  theme_ashby() +
  theme(
    axis.line.x = element_line(colour = "grey92"),
    axis.text.y = element_blank(),
    legend.justification = c(0, 1),
    legend.position = c(0.05, 0.95),
    # legend.title = element_blank(),
    panel.grid.major.x = element_line(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_line(linetype = "21"),
    panel.grid.minor.y = element_blank()
  )

ggsave("figure_14-5.eps", width = 7, height = 7 * 0.5, units = "in")
ggsave("figure_14-5.pdf", width = 7, height = 7 * 0.5, units = "in")

# ggsave("figure_14-5.pdf", width = 159, height = 159 * 0.6, units = "mm")
# ggsave("figure_14-5.tiff", width = 7, height = 7 * 0.6, units = "in", 
#        dpi = 800)
```


Time is often relevant to spatial analysis because spatial crime patterns frequently vary over time. Figure \@ref(fig:districts) shows the frequency of assaults with a dangerous weapon throughout the week in two districts of Washington, DC. The First District includes Capitol Hill, half the National Mall and the Southwest Federal Center government buildings, and some residential areas. The Seventh District is almost entirely residential except for a large military base. Figure \@ref(fig:districts) shows similarities between weekly patterns of serious assaults in both districts, but with notable differences. Dangerous-weapon assaults in the First District are clustered on Friday and Saturday evenings, whereas in the Seventh District offenses peak the in late evening on every weeknight, with some additional peaks during afternoons.

Variations in temporal patterns also exists at smaller geographies. Figure \@ref(fig:hotspots) shows hotspots of assaults with dangerous weapons in the area around the White House at different times^[Hotspots were identified by counting assaults in each 500-metre-wide cell in a hexagonal grid, then calculating Getis–Ord $G_i^*$ [@Getis:1992hw] for each cell. $Z$ scores greater than $3.09$ (equivalent to $p = 0.001$) are treated as significant.].
<!-- While some hotspots are persistent across all times of day, others exist only at certain times.  -->
Farragut Square (marked A on Figure \@ref(fig:hotspots)) is a significant hotspot only during the daytime, unsurprisingly since it hosts many large businesses that generate daytime foot traffic. The Penn Quarter/Chinatown neighborhood (marked B), which hosts many restaurants and entertainment venues, is a hotspot both in the daytime and the evening. Meanwhile, Dupont Circle (marked C) is a hotspot only overnight.


```{r prepare hotspots}
# see https://rpubs.com/quarcs-lab/spatial-autocorrelation for help

# create grid
district1_grid <- districts %>% 
  # filter(DISTRICT == 1) %>% 
  st_make_grid(cellsize = 500, square = FALSE) %>% 
  st_as_sf() %>% 
  mutate(cell_num = row_number())

# create notable features
map_features <- tribble(
  ~name, ~x, ~y,
  # "Capitol Building", -77.0091, 38.8898,
  "White House", -77.0365, 38.8977
) %>% 
  st_as_sf(crs = 4326, coords = c("x", "y")) %>% 
  st_transform(crs = msp_proj)

# create hotspot labels
map_labels <- tribble(
  ~name, ~x, ~y, ~time_group,
  "A", -77.0395, 38.9001, "daytime (07:00 to 17:59)",
  "B", -77.0281, 38.8967, "daytime (07:00 to 17:59)",
  "B", -77.0281, 38.8967, "evening (18:00 to 00:59)",
  "C", -77.0439, 38.9056, "overnight (01:00 to 06:59)"
) %>% 
  st_as_sf(crs = 4326, coords = c("x", "y")) %>% 
  st_transform(crs = msp_proj)

# set bbox for map
map_boundary <- map_features %>% 
  filter(name == "White House") %>% 
  st_buffer(1000 * 3.3) %>%  # converts feet to metres
  st_bbox()

# get main streets
map_streets <- map_features %>% 
  filter(name == "White House") %>% 
  st_buffer(2000 * 3.3) %>%  # converts feet to metres
  st_transform(4326) %>% 
  st_bbox() %>% 
  osmdata::opq() %>% 
  osmdata::add_osm_feature(key = "highway") %>% 
  osmdata::osmdata_sf() %>% 
  pluck("osm_lines") %>% 
  st_transform(crs = msp_proj) %>% 
  filter(highway %in% c(
    "motorway", "motorway_link", "trunk", "trunk_link", "primary", 
    "primary_link", "secondary", "secondary_link" # , "tertiary", "tertiary_link"
  ))

# get surface water
map_water <- map_features %>% 
  filter(name == "White House") %>% 
  st_buffer(2000 * 3.3) %>%  # converts feet to metres
  st_transform(4326) %>% 
  st_bbox() %>% 
  osmdata::opq() %>% 
  osmdata::add_osm_feature(key = "natural", value = "water") %>% 
  osmdata::osmdata_sf()
map_water <- rbind(
  select(map_water$osm_polygons, osm_id, name, water, geometry), 
  select(map_water$osm_multipolygons, osm_id, name, water, geometry)
) %>%
  st_transform(crs = msp_proj)

# create weights matrix
cell_wts <- district1_grid %>% 
  # extract centroid coordinates
  st_geometry() %>% 
  st_centroid() %>% 
  # create matrix of neighbour relationships
  spdep::dnearneigh(0, 850) %>% 
  # include each cell as one of its own neighbours
  spdep::include.self() %>% 
  # convert the relationships to weights (binary in this case)
  spdep::nb2listw(style = "B")

# prepare data
hotspot_data <- data %>% 
  filter(offense %in% c("ROBBERY")) %>% 
  mutate(time_group = case_when(
    from_time >= hm("01:00") & from_time < hm("07:00") ~ 
      "overnight (01:00 to 06:59)",
    from_time >= hm("07:00") & from_time < hm("18:00") ~ 
      "daytime (07:00 to 17:59)",
    (from_time >= hm("18:00") & from_time <= hm("23:59")) | 
      (from_time >= hm("00:00") & from_time < hm("01:00")) ~ 
      "evening (18:00 to 00:59)",
    TRUE ~ NA_character_
  )) %>% 
  select(from_time, time_group, xcoord, ycoord) %>% 
  nest(data = c(from_time, xcoord, ycoord)) %>% 
  mutate(
    counts = map(data, function (x) {
      x %>% 
        # convert tibble to an SF object
        st_as_sf(crs = msp_proj, coords = c("xcoord", "ycoord")) %>% 
        # count crimes in each grid cell
        st_join(district1_grid, join = st_within) %>% 
        as_tibble() %>% 
        count(cell_num) %>% 
        right_join(district1_grid, by = "cell_num") %>% 
        replace_na(list(n = 0)) %>% 
        select(cell_num, crimes = n, geometry = x) %>% 
        # calculate GI*
        mutate(gistar = as.numeric(spdep::localG(crimes, cell_wts)))
    })
  ) %>% 
  select(-data) %>% 
  unnest(cols = "counts") %>% 
  st_as_sf(crs = msp_proj) %>% 
  # filter out cells with non-significant crime counts
  filter(gistar > 3.0903)
```


(ref:hotspots) Hotspots of assault with a dangerous weapon at different times of day in Washington, DC, 2016

```{r hotspots, include=TRUE, fig.cap="(ref:hotspots)"}
# plot map
ggplot() + 
  # Potomac river and other water features
  geom_sf(data = map_water, colour = NA, fill = "grey92") +
  # street grid
  geom_sf(data = map_streets, colour = "grey88") +
  # hotspots
  geom_sf(
    aes(fill = "hotspot", geometry = geometry),
    data = mutate(group_by(hotspot_data, time_group), 
                  geometry = st_union(geometry)),
    colour = NA,
    alpha = 0.5
  ) +
  # hotspot outlines
  geom_sf(
    aes(colour = "hotspot", geometry = geometry),
    data = mutate(group_by(hotspot_data, time_group),
                  geometry = st_union(geometry)),
    fill = NA
  ) +
  # points of interest
  geom_sf(
    aes(shape = name),
    data = map_features,
    colour = "black",
    fill = "black",
    size = 2
  ) +
  # hotspot labels
  geom_sf_text(
    aes(label = name),
    data = map_labels,
    colour = chart_elements$label_text_colour,
    fontface = "bold",
    lineheight = chart_elements$label_text_lineheight,
    size = chart_elements$label_text_size * 1.2,
    hjust = 1,
    vjust = 1
  ) +
  # north arrow
  ggspatial::annotation_north_arrow(
    data = tibble(time_group = "overnight (01:00 to 06:59)"),
    location = "br",
    height = unit(1, "lines"), 
    width = unit(1, "lines"),
    style = ggspatial::north_arrow_fancy_orienteering(
      line_col = "grey50", fill = "grey50", text_size = 0)
  ) +
  # scale bar
  ggspatial::annotation_scale(
    data = tibble(time_group = "overnight (01:00 to 06:59)"),
    location = "bl",
    height = unit(0.33, "lines"), 
    line_col = "grey40",
    text_col = "grey40",
    style = "ticks", 
    unit_category = "metric", 
    width_hint = 0.2
  ) +
  scale_colour_manual(
    name = "hotspot", 
    values = c("hotspot" = "grey25"),
    labels = c("hotspot" = "crime hotspot")
  ) +
  scale_fill_manual(
    name = "hotspot", 
    values = c("hotspot" = "grey75"),
    labels = c("hotspot" = "crime hotspot")
  ) +
  scale_shape_manual(values = c("Capitol Building" = 23, "White House" = 8)) +
  facet_wrap(vars(time_group)) +
  lims(
    x = c(map_boundary$xmin, map_boundary$xmax),
    y = c(map_boundary$ymin, map_boundary$ymax)
  ) +
  labs(
    caption = "Data from OpenStreetMap: www.openstreetmap.org/copyright"
  ) +
  theme_ashby() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.box.background = element_rect(colour = NA, fill = "white"),
      # element_rect(colour = NA, fill = alpha("white", 0.75)),
    legend.box.margin = margin(0, 4, 4, 4, "pt"),
    legend.justification = c(0, 0),
    legend.margin = margin(0, 0, 0, 0, "pt"),
    legend.position = c(0.01, 0.01),
    legend.spacing = unit(0, "lines"),
    legend.title = element_blank(),
    panel.border = element_rect(colour = "grey70", fill = NA),
    panel.grid = element_blank(),
    plot.caption = element_text(size = 8)
  )

ggsave("figure_14-6.eps", width = 7, height = 7 * 0.5, units = "in", device = cairo_ps)
ggsave("figure_14-6.pdf", width = 7, height = 7 * 0.5, units = "in", device = cairo_pdf)
```

Ignoring the temporal dimension of crime and place risks committing an ecological fallacy. This can be avoided by using an appropriate theoretical perspective to consider the role of time in a given situation. The routine activities approach can identify how the aggregate activity patterns of different actors in a crime event might influence the spatio-temporal distribution of crime [@Eck:2015aa]. For example, crime in a city park may require analysis that distinguishes crimes during daylight and at night, or crimes on weekdays and at weekends. The multi-faceted nature of temporal crime variation (apparent in Figure \@ref(fig:scales)) means it is often fruitful to explore empirically multiple ways of 'slicing' data to understand how time influences spatial crime patterns.



# References

